{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji as emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emoji.EMOJI_UNICODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":beaming_face_with_smiling_eyes:\",\n",
    "                    \"3\": \":downcast_face_with_sweat:\",\n",
    "                    \"4\": \":fork_and_knife:\",\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_emoji.csv\",header=None)\n",
    "test = pd.read_csv(\"test_emoji.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>never talk to me again</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am proud of your achievements</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is the worst day in my life</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miss you so much</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>food is life</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0  1   2     3\n",
       "0           never talk to me again  3 NaN   NaN\n",
       "1  I am proud of your achievements  2 NaN   NaN\n",
       "2   It is the worst day in my life  3 NaN   NaN\n",
       "3                 Miss you so much  0 NaN   [0]\n",
       "4                     food is life  4 NaN   NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üç¥'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.emojize(\":fork_and_knife:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ù§Ô∏è\n",
      "‚öæ\n",
      "üòÅ\n",
      "üòì\n",
      "üç¥\n"
     ]
    }
   ],
   "source": [
    "for e in emoji_dictionary.values():\n",
    "    print(emoji.emojize(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.values\n",
    "dtest = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never talk to me again üòì\n",
      "I am proud of your achievements üòÅ\n",
      "It is the worst day in my life üòì\n",
      "Miss you so much ‚ù§Ô∏è\n",
      "food is life üç¥\n",
      "I love you mum ‚ù§Ô∏è\n",
      "Stop saying bullshit üòì\n",
      "congratulations on your acceptance üòÅ\n",
      "The assignment is too long  üòì\n",
      "I want to go play ‚öæ\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(data[i][0],emoji.emojize(emoji_dictionary[str(data[i][1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = train[0]\n",
    "Xtest = test[0]\n",
    "\n",
    "Ytrain = to_categorical(train[1])\n",
    "Ytest = to_categorical(test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"glove.6B.50d.txt\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float')\n",
    "    embeddings_index[word]=coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.4295e-01, -4.2946e-01, -5.4277e-01, -1.0307e+00,  1.2056e+00,\n",
       "       -2.7174e-01, -6.3561e-01, -1.5065e-02,  3.7856e-01,  4.6474e-02,\n",
       "       -1.3102e-01,  6.0500e-01,  1.6391e+00,  2.3940e-01,  1.2128e+00,\n",
       "        8.3178e-01,  7.3893e-01,  1.5200e-01, -1.4175e-01, -8.8384e-01,\n",
       "        2.0829e-02, -3.2545e-01,  1.8035e+00,  1.0045e+00,  5.8484e-01,\n",
       "       -6.2031e-01, -4.3296e-01,  2.3562e-01,  1.3027e+00, -8.1264e-01,\n",
       "        2.3158e+00,  1.1030e+00, -6.0608e-01,  1.0101e+00, -2.2426e-01,\n",
       "        1.8908e-02, -1.0931e-01,  3.8350e-01,  7.7362e-01, -8.1927e-02,\n",
       "       -3.4040e-01, -1.5143e-03, -5.6640e-02,  8.7359e-01,  1.4805e+00,\n",
       "        6.9421e-01, -3.0966e-01, -9.0826e-01,  3.7277e-03,  8.4550e-01])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['eat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_output(X):\n",
    "    maxLen = 10\n",
    "    embedding_out= np.zeros((X.shape[0],maxLen,50))\n",
    "    \n",
    "    for ix in range(X.shape[0]):\n",
    "        X[ix]=X[ix].split()\n",
    "        \n",
    "        for ij in range(len(X[ix])):\n",
    "            try:\n",
    "                embedding_out[ix][ij]=embeddings_index[X[ix][ij].lower()]\n",
    "            except:\n",
    "                embedding_out[ix][ij]=np.zeros((50,0))\n",
    "                \n",
    "    return embedding_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix_train = embedding_output(Xtrain)\n",
    "embeddings_matrix_test = embedding_output(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['never', 'talk', 'to', 'me', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 10, 50)\n",
      "(56, 10, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_matrix_train.shape)\n",
    "print(embeddings_matrix_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10, 64)            29440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 62,789\n",
      "Trainable params: 62,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64,input_shape=(10,50),return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64,return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 105 samples, validate on 27 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 1.6736 - accuracy: 0.1333 - val_loss: 1.5963 - val_accuracy: 0.1481\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59630, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.6166 - accuracy: 0.2656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 3ms/step - loss: 1.6240 - accuracy: 0.2190 - val_loss: 1.5953 - val_accuracy: 0.2593\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.59630 to 1.59529, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 1.5732 - accuracy: 0.3048 - val_loss: 1.6033 - val_accuracy: 0.2593\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.59529\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 1.5341 - accuracy: 0.3429 - val_loss: 1.6170 - val_accuracy: 0.2222\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.59529\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 1.5223 - accuracy: 0.3619 - val_loss: 1.6334 - val_accuracy: 0.2222\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.59529\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 1.4921 - accuracy: 0.3333 - val_loss: 1.6527 - val_accuracy: 0.2222\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.59529\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 1.4703 - accuracy: 0.3429 - val_loss: 1.6729 - val_accuracy: 0.2222\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.59529\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 1.4587 - accuracy: 0.3810 - val_loss: 1.6870 - val_accuracy: 0.2222\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.59529\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 1.4598 - accuracy: 0.3905 - val_loss: 1.6897 - val_accuracy: 0.2593\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.59529\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 1.4181 - accuracy: 0.4571 - val_loss: 1.6795 - val_accuracy: 0.2963\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.59529\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 1.3931 - accuracy: 0.4762 - val_loss: 1.6595 - val_accuracy: 0.2963\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.59529\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 1.3551 - accuracy: 0.4476 - val_loss: 1.6357 - val_accuracy: 0.2593\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.59529\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 1.3336 - accuracy: 0.5238 - val_loss: 1.6015 - val_accuracy: 0.2963\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.59529\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 1.2686 - accuracy: 0.5333 - val_loss: 1.5636 - val_accuracy: 0.2593\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.59529 to 1.56360, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 1.2438 - accuracy: 0.5143 - val_loss: 1.5218 - val_accuracy: 0.2593\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.56360 to 1.52184, saving model to best_model.h5\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 1.1898 - accuracy: 0.5524 - val_loss: 1.4665 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.52184 to 1.46655, saving model to best_model.h5\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 1.1480 - accuracy: 0.6095 - val_loss: 1.4091 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.46655 to 1.40908, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 1.0856 - accuracy: 0.6095 - val_loss: 1.3605 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.40908 to 1.36054, saving model to best_model.h5\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 1.0453 - accuracy: 0.5810 - val_loss: 1.3231 - val_accuracy: 0.3704\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.36054 to 1.32308, saving model to best_model.h5\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.9447 - accuracy: 0.7048 - val_loss: 1.2971 - val_accuracy: 0.5185\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.32308 to 1.29706, saving model to best_model.h5\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.8321 - accuracy: 0.7143 - val_loss: 1.2752 - val_accuracy: 0.5185\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.29706 to 1.27521, saving model to best_model.h5\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.8079 - accuracy: 0.7238 - val_loss: 1.2652 - val_accuracy: 0.4444\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.27521 to 1.26516, saving model to best_model.h5\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.7699 - accuracy: 0.7333 - val_loss: 1.2471 - val_accuracy: 0.4815\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.26516 to 1.24712, saving model to best_model.h5\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.6777 - accuracy: 0.8190 - val_loss: 1.2160 - val_accuracy: 0.5185\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.24712 to 1.21597, saving model to best_model.h5\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.6366 - accuracy: 0.7524 - val_loss: 1.1832 - val_accuracy: 0.4815\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.21597 to 1.18319, saving model to best_model.h5\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.6312 - accuracy: 0.7905 - val_loss: 1.0819 - val_accuracy: 0.4444\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.18319 to 1.08187, saving model to best_model.h5\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.5454 - accuracy: 0.8381 - val_loss: 1.0265 - val_accuracy: 0.5556\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.08187 to 1.02653, saving model to best_model.h5\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.4799 - accuracy: 0.8476 - val_loss: 0.9329 - val_accuracy: 0.6296\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.02653 to 0.93287, saving model to best_model.h5\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.4267 - accuracy: 0.8952 - val_loss: 0.9313 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.93287 to 0.93130, saving model to best_model.h5\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.4511 - accuracy: 0.8476 - val_loss: 1.0033 - val_accuracy: 0.6296\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.93130\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.8762 - val_loss: 0.9613 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.93130\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.4053 - accuracy: 0.8857 - val_loss: 0.9534 - val_accuracy: 0.6296\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.93130\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3919 - accuracy: 0.8762 - val_loss: 1.0488 - val_accuracy: 0.6296\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.93130\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3305 - accuracy: 0.8667 - val_loss: 1.1310 - val_accuracy: 0.5926\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.93130\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3423 - accuracy: 0.8857 - val_loss: 0.9927 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.93130\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.3163 - accuracy: 0.9143 - val_loss: 0.9903 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.93130\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2696 - accuracy: 0.9143 - val_loss: 1.0288 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.93130\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.3317 - accuracy: 0.8857 - val_loss: 0.9871 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.93130\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2356 - accuracy: 0.9238 - val_loss: 0.9319 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.93130\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.3120 - accuracy: 0.8857 - val_loss: 1.0125 - val_accuracy: 0.6296\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.93130\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9429 - val_loss: 1.1196 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.93130\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2607 - accuracy: 0.9333 - val_loss: 1.1693 - val_accuracy: 0.6296\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.93130\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1719 - accuracy: 0.9619 - val_loss: 1.0815 - val_accuracy: 0.6296\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.93130\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9333 - val_loss: 1.0656 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.93130\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.9429 - val_loss: 1.1216 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.93130\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1374 - accuracy: 0.9810 - val_loss: 1.1063 - val_accuracy: 0.5926\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.93130\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.9619 - val_loss: 1.0732 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.93130\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9714 - val_loss: 1.0232 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.93130\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1156 - accuracy: 0.9714 - val_loss: 1.0308 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.93130\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9714 - val_loss: 1.0415 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.93130\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.9619 - val_loss: 1.0482 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.93130\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0663 - accuracy: 0.9905 - val_loss: 1.1066 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.93130\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0985 - accuracy: 0.9714 - val_loss: 1.1692 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.93130\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 1.0000 - val_loss: 1.1852 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.93130\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 0.9810 - val_loss: 1.1722 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.93130\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0632 - accuracy: 0.9905 - val_loss: 1.1624 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.93130\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0630 - accuracy: 0.9810 - val_loss: 1.2106 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.93130\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 1.0000 - val_loss: 1.2305 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.93130\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0497 - accuracy: 1.0000 - val_loss: 1.2499 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.93130\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 1.2405 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.93130\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 1.1943 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.93130\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 1.1192 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.93130\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 1.0793 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.93130\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0424 - accuracy: 0.9905 - val_loss: 1.2060 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.93130\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0401 - accuracy: 0.9905 - val_loss: 1.1581 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.93130\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.9806 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.93130\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9810 - val_loss: 0.9747 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.93130\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 1.0262 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.93130\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9905 - val_loss: 1.1088 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.93130\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0484 - accuracy: 0.9905 - val_loss: 1.0240 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.93130\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.9897 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.93130\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 1.0636 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.93130\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0438 - accuracy: 0.9810 - val_loss: 1.3746 - val_accuracy: 0.5926\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.93130\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0358 - accuracy: 0.9905 - val_loss: 1.5729 - val_accuracy: 0.5556\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.93130\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 1.5707 - val_accuracy: 0.5926\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.93130\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9905 - val_loss: 1.5749 - val_accuracy: 0.5926\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.93130\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 1.5660 - val_accuracy: 0.5926\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.93130\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 1.5767 - val_accuracy: 0.5926\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.93130\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0303 - accuracy: 0.9905 - val_loss: 1.5905 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.93130\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0358 - accuracy: 0.9905 - val_loss: 1.4263 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.93130\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 1.3100 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.93130\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0655 - accuracy: 0.9810 - val_loss: 1.2437 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.93130\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9714 - val_loss: 1.2923 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.93130\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 1.5345 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.93130\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9619 - val_loss: 1.1098 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.93130\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.9905 - val_loss: 0.9497 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.93130\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 1.1308 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.93130\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 1.1172 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.93130\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0286 - accuracy: 0.9905 - val_loss: 1.1366 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.93130\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 1.0253 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.93130\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 1.2485 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.93130\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0507 - accuracy: 0.9810 - val_loss: 1.3952 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.93130\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 1.7873 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.93130\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0995 - accuracy: 0.9619 - val_loss: 1.5887 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.93130\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0633 - accuracy: 0.9810 - val_loss: 1.1995 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.93130\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0221 - accuracy: 0.9905 - val_loss: 1.2523 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.93130\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1331 - accuracy: 0.9619 - val_loss: 0.9166 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.93130 to 0.91659, saving model to best_model.h5\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0313 - accuracy: 0.9905 - val_loss: 1.0865 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.91659\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 1.1193 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.91659\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0474 - accuracy: 0.9810 - val_loss: 0.9852 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.91659\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"best_model.h5\",monitor='val_loss',verbose=True,save_best_only=True)\n",
    "earlystop = EarlyStopping(monitor='val_acc',patience=10)\n",
    "\n",
    "hist = model.fit(embeddings_matrix_train,Ytrain,batch_size=64,epochs=100,shuffle=True,\n",
    "                 validation_split=0.2,callbacks=[checkpoint,earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4617455516542708, 0.6964285969734192]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(embeddings_matrix_test,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(embeddings_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to eat\n",
      "üç¥\n",
      "üç¥\n",
      "he did not answer\n",
      "üòì\n",
      "üòì\n",
      "he got a raise\n",
      "üòÅ\n",
      "üòì\n",
      "she got me a present\n",
      "‚ù§Ô∏è\n",
      "‚ù§Ô∏è\n",
      "ha ha ha it was so funny\n",
      "üòÅ\n",
      "üòÅ\n",
      "he is a good friend\n",
      "‚ù§Ô∏è\n",
      "üòÅ\n",
      "I am upset\n",
      "‚ù§Ô∏è\n",
      "üòì\n",
      "We had such a lovely dinner tonight\n",
      "‚ù§Ô∏è\n",
      "üòÅ\n",
      "where is the food\n",
      "üç¥\n",
      "üç¥\n",
      "Stop making this joke ha ha ha\n",
      "üòÅ\n",
      "üòÅ\n",
      "where is the ball\n",
      "‚öæ\n",
      "‚öæ\n",
      "work is hard\n",
      "üòì\n",
      "üòÅ\n",
      "This girl is messing with me\n",
      "üòì\n",
      "‚ù§Ô∏è\n",
      "are you serious ha ha\n",
      "üòÅ\n",
      "üòì\n",
      "Let us go play baseball\n",
      "‚öæ\n",
      "‚öæ\n",
      "This stupid grader is not working\n",
      "üòì\n",
      "üòì\n",
      "work is horrible\n",
      "üòì\n",
      "üòì\n",
      "Congratulation for having a baby\n",
      "üòÅ\n",
      "üòÅ\n",
      "stop messing around\n",
      "üòì\n",
      "üòì\n",
      "any suggestions for dinner\n",
      "üç¥\n",
      "üç¥\n",
      "I love taking breaks\n",
      "‚ù§Ô∏è\n",
      "‚ù§Ô∏è\n",
      "you brighten my day\n",
      "üòÅ\n",
      "‚ù§Ô∏è\n",
      "I boiled rice\n",
      "üç¥\n",
      "üç¥\n",
      "she is a bully\n",
      "üòì\n",
      "üòì\n",
      "Why are you feeling bad\n",
      "üòì\n",
      "üòì\n",
      "I am upset\n",
      "üòì\n",
      "üòì\n",
      "I worked during my birthday\n",
      "üòì\n",
      "üòÅ\n",
      "My grandmother is the love of my life\n",
      "‚ù§Ô∏è\n",
      "‚ù§Ô∏è\n",
      "enjoy your break\n",
      "üòÅ\n",
      "‚öæ\n",
      "valentine day is near\n",
      "‚ù§Ô∏è\n",
      "üòÅ\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(' '.join(Xtest[i]))\n",
    "    print(emoji.emojize(emoji_dictionary[str(np.argmax(Ytest[i]))]))\n",
    "    print(emoji.emojize(emoji_dictionary[str(pred[i])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
